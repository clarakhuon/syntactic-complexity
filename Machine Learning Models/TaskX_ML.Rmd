---
title: "Task X - Machine Learning"
author: "Clara Khuon"
date: "January 15th, 2026"
output: html_document
---

## Task X with PHQ-9 Scores at Timepoint 4 ##

```{r setup, include=FALSE}
# Set working directory
setwd("...")

# Load libraries
library(tidyverse)
library(here)
library(caret)
library(nestedcv)
library(doParallel)
library(ggplot2)
library(fastshap)

# Load data
demographic_data <- read_csv(file = here("demographic_data.csv"))
taskX_TAASSC <- read_csv(file = here("taskX_linguistic_data_TAASSC.csv"))
taskX_SCA <- read_csv(file = here("taskX_linguistic_data_SCA.csv"))

# Note: File names above are example placeholders

```

# Preprocessing
```{r}
# Filter demographic dataset to specific variables
taskX_demographic_variables <- c("UserID_v2", "T4_PHQ9_Total", "Age", "Gender", "Education", "Diagnosis_Onset")
taskX_demographic_data <- demographic_data[, taskX_demographic_variables]

# Order SCA dataset in ascending order of participant ID number
taskX_SCA$filename <- basename(taskX_TAASSC$filename)
taskX_SCA$filename <- str_remove(taskX_SCA$filename,".txt")
taskX_SCA$filename <- str_remove(taskX_SCA$filename,"TaskX.Participant")
taskX_SCA <- taskX_SCA[order(as.numeric(taskX_SCA$filename), decreasing = FALSE), ]
taskX_SCA$filename <- paste("Participant.", taskX_SCA$filename, sep = "")

# Order TAASSC dataset in ascending order of participant ID number
taskX_TAASSC$filename <- str_remove(taskX_TAASSC$filename,".txt")
taskX_TAASSC$filename <- str_remove(taskX_TAASSC$filename,"TaskX.Participant")
taskX_TAASSC <- taskX_TAASSC[order(as.numeric(taskX_TAASSC$filename), decreasing = FALSE), ]
taskX_TAASSC$filename <- paste("Participant.", taskX_TAASSC$filename, sep = "")

# Remove predictor variables with near-zero variance from TAASSC dataset
near_zero_table <- nearZeroVar(taskX_TAASSC, saveMetrics = TRUE)
near_zero_TAASSC <- nearZeroVar(taskX_TAASSC) 
taskX_TAASSC <- taskX_TAASSC[,-near_zero_TAASSC]

# Combine demographic dataset with TAASCC and SCA datasets (without duplicate participant ID number and word-count columns)
taskX_total <- cbind(taskX_demographic_data, taskX_TAASSC, taskX_SCA[,3:16])

# Identify and remove participants with invalid writing samples (i.e., word-count of zero)
invalid <- filter(taskX_total, nwords == 0)
taskX_filtered <- filter(taskX_total, nwords != 0)

# Remove second ID column
taskX_filtered <- taskX_filtered[,-which(colnames(taskX_filtered) == "filename")]

# Define gender and education as categorical variables in final dataset
taskX_filtered$Gender <- as.factor(taskX_filtered$Gender)
taskX_filtered$Education <- as.factor(taskX_filtered$Education)

# Final dataset
taskX_final <- taskX_filtered
taskX_final

```

# Elastic Net with Nested Cross-Validation
```{r}
# Start timer
start_time <- Sys.time()

# Enable parallel processing
cl <- makeCluster(detectCores(), type = "PSOCK")
registerDoParallel(cl)

# Set seed
set.seed(seed = 123)

# Fit and tune elastic net model with nested cross-validation
taskX_elastic_net <- nestcv.train(x = taskX_final[,7:307], # only linguistic variables
                                  y = taskX_final$T4_PHQ9_Total,  
                     method = "glmnet",
                     metric = "RMSE",
                     savePredictions = "final",
                     filterFUN = "correl_filter",
                     n_outer_folds = 10,
                     n_inner_folds = 10,
                     finalCV = TRUE,
                     preProcess = c("center", "scale", "corr"),
                     preProcOptions = list(cutoff = 0.75),
                     tuneGrid = expand.grid(alpha = seq(0,1,0.1),
                                            lambda = 2^c(-10:10)))

# End timer
end_time <- Sys.time() 
end_time - start_time

# Hyperparameters of final model
taskX_elastic_net$finalTune

# Summarise final results
max(taskX_elastic_net$final_fit$results$Rsquared,na.rm = TRUE) #r-squared of best parameters fit to the whole dataset
taskX_elastic_net$summary # accounting for overfitting

# Frequency with which predictor variables are kept in final model
var_stability(taskX_elastic_net)
taskX_elastic_net_var_stability <- plot_var_stability(taskX_elastic_net, final = FALSE, percent = FALSE, top = 10) 
taskX_elastic_net_var_stability

# Variables in final model fit
taskX_elastic_net$final_fit # number of variables 
taskX_elastic_net$final_vars # names of variables

```

# Plot Elastic Net Predicted vs. Observed Values
```{r}

# Plot predictions 
predict_taskX_elastic_net <- predict(taskX_elastic_net, taskX_final[,c(2,7:307)])

# Create lm model to plot
model_taskX_elastic_net <- lm(taskX_elastic_net$output$testy ~ taskX_elastic_net$output$predy)
                        
# Plot result
taskX_elastic_net_predictions <- model_taskX_elastic_net %>%
  ggplot(mapping = aes(x = taskX_elastic_net$output$testy,
                       y = taskX_elastic_net$output$predy)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = lm) +
  theme_bw() +
  labs(title = "Task X: Elastic Net",
       subtitle = "Linguistic variables only",
       x = "Observed PHQ-9 Score (Total)",
       y = "Predicted PHQ-9 Score (Total)")

# Display plot
taskX_elastic_net_predictions

```

# Plot Coefficients of Final Elastic Net Model
```{r}
# Access the final nestedcv model
final_model_taskX_elastic_net <- taskX_elastic_net$final_fit$finalModel

#Identify optimal lambda 
best_lambda <- final_model_taskX_elastic_net$lambdaOpt

# Extract coefficients
coefficients <- coef(final_model_taskX_elastic_net, s = best_lambda)
coef_df <- as.data.frame(as.matrix(coefficients))
names(coef_df) <- "Coefficient"
coef_df$Variable <- rownames(coef_df)

# Remove intercept and zero coefficients
coef_df <- coef_df[-1, ]  # First row is the intercept
coef_df <- coef_df[coef_df$Coefficient != 0, ]  # Remove zero coefficients

# Calculate absolute values for ordering and direction
coef_df$AbsoluteValue <- abs(coef_df$Coefficient)

# Order by absolute value, highest first
coef_df <- coef_df[order(-coef_df$AbsoluteValue), ]

# Plot using ggplot2
ggplot(coef_df[1:10,], aes(x = reorder(Variable, AbsoluteValue), y = AbsoluteValue, fill = Coefficient > 0)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +  # Make the bar plot horizontal
  scale_fill_manual(values = c("orange", "lightblue"), labels = c("Negative", "Positive"), name = "Direction") +  # Orange for negative, blue for positive
  labs(x = "", y = "Coefficient Value", title = "Task X: Elastic Net Final Model") +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 10, color = "black"),
        axis.text.y = element_text(size = 10, color = "black"),
        axis.line = element_line(color = "black"),
        axis.ticks = element_line(color = "black"), 
        axis.ticks.length = unit(0.15, "cm")) 

```

# Random Forest with Nested Cross-Validation
```{r}
# Enable parallel processing
cl <- makeCluster(detectCores(), type = "PSOCK")
registerDoParallel(cl)

start_time <- Sys.time()

# set seed
set.seed(seed = 123)

# Random forest with nested cross-validation
taskX_rf <- nestcv.train(x = taskX_final[,7:307], 
                         y = taskX_final$T4_PHQ9_Total,  
                     method = "rf",
                     metric = "RMSE",
                     savePredictions = "final",
                     filterFUN = "correl_filter",
                     n_outer_folds = 10,
                     n_inner_folds = 10,
                     finalCV = TRUE,
                     preProcess = c("center", "scale", "corr"),
                     preProcOptions = list(cutoff = 0.75),
                     tuneGrid = expand.grid(mtry = 1:40))

end_time <- Sys.time()
end_time - start_time

# Hyperparameters of final model
taskX_rf$finalTune

# Results summary
max(taskX_rf$final_fit$results$Rsquared,na.rm = TRUE) # r-squared of best parameters fit to the whole dataset
taskX_rf$summary # accounting for overfitting

# Frequency with which predictor variables are kept in final model
var_stability(taskX_rf)
taskX_rf_var_stability <- plot_var_stability(taskX_rf, final = FALSE, top = 10) 
taskX_rf_var_stability

# Variables in final model fit
taskX_rf$final_fit # number of variables 
taskX_rf$final_vars # names of variables

```

# Random Forest Plot Predicted vs. Observed Values
```{r}
# Plot predictions 
predict_taskX_rf <- predict(taskX_rf, taskX_final[,c(2,7:307)])

# Create lm model to plot
model_taskX_rf <- lm(taskX_rf$output$testy ~ taskX_rf$output$predy)
                        
# Plot result
taskX_rf_predictions <- model_taskX_rf %>%
  ggplot(mapping = aes(x = taskX_rf$output$testy,
                       y = taskX_rf$output$predy)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = lm) +
  theme_bw() +
  labs(title = "Task X: Random Forest",
       subtitle = "Linguistic variables only",
       x = "Observed PHQ-9 Score (Total)",
       y = "Predicted PHQ-9 Score (Total)")

# Display plot
taskX_rf_predictions

```

# Plot SHAP Values of Final Random Forest Model
Simulations = 1000
```{r}
# Start timer
start_time <- Sys.time()

shap <- fastshap::explain(taskX_rf, X = taskX_final[,7:307], pred_wrapper = pred_train, nsim = 1000)

plot_shap_bar(shap, taskX_final[,7:307], top = 10)

# End timer
end_time <- Sys.time() 
end_time - start_time

```

# Create Beeswarm Plot
```{r}
plot_shap_beeswarm(shap, taskX_final[,7:307], size = 1, top = 10)

```

# Support Vector Machine with Nested Cross-Validation
```{r}
# Enable parallel processing
cl <- makeCluster(detectCores(), type = "PSOCK")
registerDoParallel(cl)

start_time <- Sys.time()

# set seed
set.seed(seed = 123)

# Support Vector Machine (Linear) with nested cross-validation
taskX_svm <- nestcv.train(x = taskX_final[,7:307],
                          y = taskX_final$T4_PHQ9_Total, 
                     method = "svmLinear",
                     metric = "RMSE",
                     savePredictions = "final",
                     filterFUN = "correl_filter",
                     n_outer_folds = 10,
                     n_inner_folds = 10,
                     finalCV = TRUE,
                     preProcess = c("center", "scale", "corr"),
                     preProcOptions = list(cutoff = 0.75),
                     tuneGrid = expand.grid(C = 2^c(-10:10)))

end_time <- Sys.time()
end_time - start_time

# Hyperparameters of final model
taskX_svm$finalTune

# Results Summary
max(taskX_svm$final_fit$results$Rsquared,na.rm = TRUE) # r-squared of best parameters fit to the whole dataset
taskX_svm$summary # accounting for overfitting

# Frequency with which predictor variables are kept in final model
var_stability(taskX_svm)
taskX_svm_var_stability <- plot_var_stability(taskX_svm, final = FALSE) 
taskX_svm_var_stability

# Variables in final model fit
taskX_svm$final_fit # number of variables 
taskX_svm$final_vars # names of variables

```
